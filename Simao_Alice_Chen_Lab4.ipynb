{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLSs1Eby_pPk"
      },
      "source": [
        "# The SVM vs. Logistic Regression Showdown\n",
        "\n",
        "In this lab, you will practice working with non-linear kernels combined with logistic regression and SVM classifiers. The goal is to compare these commonly used techniques. Which comes out on top in terms of accuracy? Runtime? Is there much of a difference at all? Is the dominance of the SVM classifier in machine learning pedagogy justified? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Msj5mwJs_pPl"
      },
      "source": [
        "## Loading the Data\n",
        "\n",
        "First, we load all the packages we'll need."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkxmug754t07"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5TKE1o5c_pPm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from sklearn.metrics.pairwise import pairwise_kernels\n",
        "import scipy\n",
        "from sklearn import svm, linear_model\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L58cF4Cy_pPn"
      },
      "source": [
        "Again we download the data from the Tensorflow package, which you will need to install.  You can get the data from other sources as well. \n",
        "\n",
        "In the Tensorflow dataset, the training and test data are represented as arrays:\n",
        "\n",
        "     Xtr.shape = 60000 x 28 x 28\n",
        "     Xts.shape = 10000 x 28 x 28\n",
        "     \n",
        "The test data consists of `60000` images of size `28 x 28` pixels; the test data consists of `10000` images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pigfivuh_pPn",
        "outputId": "a6dc68ca-fac8-4441-98aa-13f4416d80e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Xtr shape: (60000, 28, 28)\n",
            "Xts shape: (10000, 28, 28)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "(Xtr_raw,ytr),(Xts_raw,yts) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "print('Xtr shape: %s' % str(Xtr_raw.shape))\n",
        "print('Xts shape: %s' % str(Xts_raw.shape))\n",
        "\n",
        "ntr = Xtr_raw.shape[0]\n",
        "nts = Xts_raw.shape[0]\n",
        "nrow = Xtr_raw.shape[1]\n",
        "ncol = Xtr_raw.shape[2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgE5TO4b_pPo"
      },
      "source": [
        "Each pixel value is from `[0,255]`.  For this lab, it will be convenient to recale the value to -1 to 1 and reshape it as a `ntr x npix` and `nts x npix`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QE-50PdU_pPo"
      },
      "outputs": [],
      "source": [
        "npix = nrow*ncol\n",
        "# print(npix)\n",
        "Xtr = (Xtr_raw/255 - 0.5)\n",
        "Xtr = Xtr.reshape((ntr,npix))\n",
        "Xts = (Xts_raw/255 - 0.5)\n",
        "Xts = Xts.reshape((nts,npix))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frhehbKs_pPo"
      },
      "source": [
        "For this lab we're only going to use a fraction of the MNIST data -- otherwise our models will take too much time and memory to run. Using only part of the training data will of course lead to worse results. Given enough computational resources and time, we would ideally be running on the full data set. The follow code creates a new test and train set, with 10000 examples for train and 5000 for test. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUHbBwL1_pPp"
      },
      "outputs": [],
      "source": [
        "ntr1 = 10000\n",
        "nts1 = 5000\n",
        "Iperm = np.random.permutation(ntr1)\n",
        "Xtr1 = Xtr[Iperm[:ntr1],:]\n",
        "ytr1 = ytr[Iperm[:ntr1]]\n",
        "Iperm = np.random.permutation(nts1)\n",
        "Xts1 = Xts[Iperm[:nts1],:]\n",
        "yts1 = yts[Iperm[:nts1]]\n",
        "# print(Xtr1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlz6g-X8_pPq"
      },
      "source": [
        "## Problem set up and establishing a baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T90pjHcr_pPq"
      },
      "source": [
        "To simplify the problem (and speed things up) we're also going to restrict to binary classification. In particular, let's try to design classifier a that separates the 8's from all other digits.\n",
        "\n",
        "Create binary 0/1 label vectors `ytr8` and `yts8` which are 1 wherever `ytr1` and `yts1` equal 8, and 0 everywhere else."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6Flo-KS_pPq"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "\n",
        "ytr8 = (ytr1 == 8).astype(int)\n",
        "yts8 = (yts1 == 8).astype(int)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdT2KGyj_pPr"
      },
      "source": [
        "Most of the digits in the test dataset aren't equal to 8. So if we simply guess 0 for every image in `Xts`, we might expect to get classification accuracy around 90%. Our goal should be to significantly beat this **baseline**. \n",
        "\n",
        "Formally, write a few lines of code to check what test error would be achieved by the all zeros classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JweYada_pPr",
        "outputId": "66b904d8-11f0-48be-dfeb-e5c414c9e941"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuaracy = 0.902200\n"
          ]
        }
      ],
      "source": [
        "# TODO\n",
        "# ...\n",
        "# acc = \n",
        "# print('Accuaracy = {0:f}'.format(acc))\n",
        "\n",
        "ypred8 = np.zeros(nts1)\n",
        "acc = np.sum((yts8 == ypred8))/nts1\n",
        "print('Accuaracy = {0:f}'.format(acc))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oz_7u10V_pPr"
      },
      "source": [
        "As a second baseline, let's see how we do with standard (non-kernel) logistic regression. As in the MNIST demo, you can use `scikit-learn`'s built in function `linear_model.LogisticRegression` to fit the model and compute the accuracy. Use no regularization and the `lbfgs` solver. You should acheive an improvement to around 93-95%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oocR_r9C_pPr",
        "outputId": "085d87a6-79ca-405b-e9ef-ee7f30e967a3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic Regression Accuaracy = 0.942200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   37.1s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   37.1s finished\n"
          ]
        }
      ],
      "source": [
        "# TODO\n",
        "# ...\n",
        "# acc = \n",
        "# print('Logistic Regression Accuaracy = {0:f}'.format(acc))\n",
        "\n",
        "\n",
        "from sklearn import linear_model\n",
        "logreg = linear_model.LogisticRegression(verbose=10, solver='lbfgs', multi_class='multinomial',max_iter=500)\n",
        "logreg.fit(Xtr1,ytr8)\n",
        "\n",
        "nts1 = 5000\n",
        "Iperm_ts = np.random.permutation(nts) \n",
        "# Xts8 = Xts[Iperm_ts[:nts8],:]\n",
        "# yts8 = yts[Iperm_ts[:nts8]]\n",
        "yhat = logreg.predict(Xts1)\n",
        "acc = np.mean(yhat == yts8)\n",
        "print('Logistic Regression Accuaracy = {0:f}'.format(acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXuJVilo_pPr"
      },
      "source": [
        "## Kernel Logistic Regression\n",
        "\n",
        "To improve on this baseline performance, let's try using the logistic regression classifier with a *non-linear* kernel. Recall from class that any non-linear kernel similarity function $k(\\vec{w},\\vec{z})$ is equal to $\\phi(\\vec{w})^T\\phi(\\vec{z})$ for some feature transformation $\\phi$. However, we typically do not need to compute this feature tranformation explicitly: instead we can work directly with the kernel gram matrix $K \\in \\mathbb{R}^{n\\times n}$. Recall that $K_{i,j} = k(\\vec{x}_i,\\vec{x}_j)$ where $\\vec{x}_i$ is the $i^\\text{th}$ training data point. \n",
        "\n",
        "For this lab we will be using the radial basis function kernel. For a given scaling factor $\\gamma$ this kernel is defined as:\n",
        "$$\n",
        "k(\\vec{w},\\vec{z}) = e^{-\\gamma\\|\\vec{w}-\\vec{z}\\|_2^2}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8znIoWi_pPs"
      },
      "outputs": [],
      "source": [
        "def rbf_kernel(w,z,gamma):\n",
        "    d = w - z\n",
        "    return np.exp(-gamma*np.sum(d*d))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WFi15JF_pPs"
      },
      "source": [
        "Construct the kernel matrix `K1` for `Xtr1` with `gamma = .05`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9PBzA-5__pPs"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "# TODO \n",
        "gamma = .05\n",
        "K1 = np.zeros((Xtr1.shape[0], Xtr1.shape[0]), dtype=float)\n",
        "for row in range(Xtr1.shape[0]):\n",
        "  for col in range(row, Xtr1.shape[0]):\n",
        "    K1[row][col] = rbf_kernel(Xtr1[row], Xtr1[col], gamma)\n",
        "    K1[col][row] = K1[row][col]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wB_X-JOR_pPs"
      },
      "source": [
        "If you used a for loop (which is fine) your code might take several minutes to run! Part of the issue is that Python won't know to properly parallize your for loop. For this reason, when constructing kernel matrices it is often faster to us a built-in, carefully optimized function with explicit parallelization. Scikit learn provides such a function through their `metrics` library. \n",
        "\n",
        "Referring to the documentation here\n",
        "https://scikit-learn.org/stable/modules/metrics.html#metrics, use this built in function to recreate the same kernel matrix you did above. Store the result at `K`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "Nhd-xFsn_pPt"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "# K = \n",
        "from sklearn.metrics import pairwise_distances\n",
        "from sklearn.metrics.pairwise import rbf_kernel as rbf \n",
        "K = rbf(Xtr1, Xtr1, gamma = 0.05)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CO9CnBbN_pPt"
      },
      "source": [
        "Check that you used the function correctly by writing code to confirm that `K = K1`, or at least that the two are equal up to very small differences (which could arise due to numerical precision issues). Try to do this **without a for loop** for full credit. You will get partial credit if you use a for loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCOfrRRD_pPt",
        "outputId": "0f0c6606-f10f-4be9-b370-1f9535a724fd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ],
      "source": [
        "# TODO\n",
        "\n",
        "# Just testing. No need to rerun\n",
        "# np.allclose(K1, K)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrIT1xJL_pPt"
      },
      "source": [
        "When using a non-linear kernel, it is important to check that you have chosen reasonable parameters (in our case the only parameter is `gamma`). We typically do not want $k(\\vec{x}_i,\\vec{x}_j)$ to be either negligably small, or very large for all $i\\neq j$ in our data set, or we won't be able to learn anything. For the RBF kernel this means that, for any $\\vec{x}_i$ we don't want $k(\\vec{x}_i,\\vec{x}_j)$ very close to 1 (e.g. .9999) for all $j$, or very close to $0$ (e.g. 1e-8) for all $j$. \n",
        "\n",
        "Let's just check that we're in good shape for the first data vector $\\vec{x}_0$. Do so by printing out the 10 largest and 10 smallest values of $k(\\vec{x}_0,\\vec{x}_j)$ for $j\\neq 0$. Note that we always have $k(\\vec{x}_0,\\vec{x}_0) = 1$ for the RBF kernel. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2yZ-vdt_pPt",
        "outputId": "1dcbda14-8a02-4d7c-f17f-c524e23112b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum similarities: \n",
            "[0.32684629 0.32921906 0.33025251 0.33890633 0.36547863 0.38108794\n",
            " 0.43343582 0.48617994 0.49642839 1.        ]\n",
            "Minimum similarities: \n",
            "[3.76381749e-05 4.14103694e-05 6.18902665e-05 6.87354366e-05\n",
            " 7.88196421e-05 7.88287943e-05 8.82068925e-05 9.44521551e-05\n",
            " 9.48388146e-05 1.11091744e-04]\n"
          ]
        }
      ],
      "source": [
        "# TODO\n",
        "K0 = np.sort(K1[0])\n",
        "print('Maximum similarities: \\n' + str(K0[-10:]))\n",
        "print('Minimum similarities: \\n' + str(K0[:10]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SuFQJt6P9HuM",
        "outputId": "62df8190-5c9a-4414-c034-d6288efeaf00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "Nl4llULm9Lo8"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "with open( \"/content/drive/MyDrive/mnist_logreg.p\", \"wb\" ) as fp:\n",
        "    pickle.dump([K1, K], fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "IZZbLgW79Vau"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "with open( \"/content/drive/MyDrive/mnist_logreg.p\", \"rb\" ) as fp:\n",
        "    K1, K = pickle.load(fp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0Vfxdfu_pPu"
      },
      "source": [
        "### Implementation\n",
        "Maybe surprisingly Scikit learn does not have an implementation for kernel logistic regression. So we have to implement our own!\n",
        "\n",
        "Write a function function `log_fit` that minimizes the $\\ell_2$-regularized logisitic regression loss:\n",
        "$$\n",
        "L(\\boldsymbol{\\alpha}) =\\sum_{i=1}^n (1-y_i)(\\phi(\\mathbf{x}_i)^T\\phi(\\mathbf{X})^T\\vec{\\alpha}) - \\log(h(\\phi(\\mathbf{x}_i)^T\\phi(\\mathbf{X})^T\\boldsymbol{\\alpha}) + \\lambda \\|\\phi(\\mathbf{X})^T\\boldsymbol{\\alpha}\\|_2^2.\n",
        "$$\n",
        "As input it takes an $n\\times n$ kernel matrix $K$ for the training data, an $n$ length vector `y` of binary class labels, and regularization parameter `lamb`.\n",
        "\n",
        "To implement your function you can either use your own implmentation of gradient descent or used a built in minimizer from `scipy.optimize.minimize`. I recommend the later approach: as we saw in the last lab, gradient descent can converge slowly for this objective."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "TKZMRi6bShZt"
      },
      "outputs": [],
      "source": [
        "import scipy \n",
        "\n",
        "def Leval_reg_linear_kernel(alpha, K, y, lamb):\n",
        "  z = K@alpha\n",
        "  h = 1/(1+np.exp(-z))\n",
        "  Lr = np.sum((1-y)*z - np.log(h)) + lamb*(alpha.T)@z\n",
        "  # Lrgrad = K@(h-y) + 2*lamb*z\n",
        "  Lrgrad = K@(h-y) + 2*lamb*z\n",
        "  return Lr, Lrgrad "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "jbG3rpY7_pPu"
      },
      "outputs": [],
      "source": [
        "# # TODO\n",
        "def log_fit(K,y,lamb,nit=1000):\n",
        "  f = lambda alpha: Leval_reg_linear_kernel(alpha, K, y, lamb)[0]\n",
        "  g = lambda alpha: Leval_reg_linear_kernel(alpha, K, y, lamb)[1]\n",
        "  alpha0 = np.zeros(ntr1)\n",
        "  res = scipy.optimize.minimize(f, alpha0, args = (), method = 'tnc', jac = g, options = {'maxIter': nit})\n",
        "  return res.x\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PC4MZYmXMAmY"
      },
      "source": [
        "Use the `log_fit` function defined above to find parameters `alpha` for the kernel logistic regression model using `lamb = 0` and `K` as constructed above (with `gamma = .05`). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNW1cYbjMBwj",
        "outputId": "26d108c0-f8a1-4f14-d9eb-032dc7763ca7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: OptimizeWarning: Unknown solver options: maxIter\n",
            "  \n"
          ]
        }
      ],
      "source": [
        "# TODO\n",
        "alpha = log_fit(K,ytr8,lamb = 0,nit=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXzFatA3V1Q6",
        "outputId": "46be63bb-5fe3-44a0-db8b-ed65f4370183"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000,)"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ],
      "source": [
        "alpha.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZXI3kz2_pPu"
      },
      "source": [
        "Suppose we have a test dataset with $m$ examples $\\vec{w}_1,\\ldots, \\vec{w}_m$. Once we obtain a coefficient vector $\\alpha$, making predictions for any $\\vec{w}_j$ in the test set requires computing:\n",
        "$$\n",
        "{y}_{j} = \\sum_{i=1}^n \\alpha_i \\cdot k(\\vec{w}_{j}, \\vec{x}_i).\n",
        "$$\n",
        "where $\\vec{x}_1, \\ldots \\vec{x}_n$ are our training data vectors. We classify $\\vec{w}_{j}$ in class 0 if ${y}_{j} \\leq 0$ and in class 1 if ${y}_{j} > 0$. \n",
        "\n",
        "This computation can be rewritten in matrix form as follows:\n",
        "$$\n",
        "\\vec{y}_{test} = K_{test}\\vec{\\alpha}, \n",
        "$$\n",
        "where $\\vec{y}_{text}$ is an $m$ length vector and $K_{test}$ is a $m\\times n$ matrix whose $(j,i)$ entry is equal to $k(\\vec{w}_{j}, \\vec{x}_i)$. We classify $\\vec{w}_{j}$ in class 0 if $\\vec{y}_{test}[j] \\leq 0$ and in class 1 if $\\vec{y}_{test}[j] > 0$. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6BhIPrH_pPv"
      },
      "source": [
        "Use the `pairwise_kernels` function to construct $K_{test}$. Then make predictions for the test set and evaluate the accuracy of our kernel logistic regression classifier. You should see a pretty substantial lift in accuracy to around $97\\%$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "KA6EwssY_pPv"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import pairwise\n",
        "# TODO \n",
        "# t = time.time()\n",
        "Ktest = pairwise_kernels(Xts1, Xtr1, metric = \"rbf\", gamma = 0.05)\n",
        "# yhat = Ktest@alpha\n",
        "# return\n",
        "# Ktest = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrAoBfvK_pPv",
        "outputId": "457bde68-1d8d-42c2-df9b-e2eae623e6e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-34.2416466  -20.15838618 -25.80371192 ... -25.99536714 -49.63691286\n",
            " -26.37014087]\n",
            "[0 0 0 ... 0 0 0]\n",
            "Test accuracy = 0.981000\n"
          ]
        }
      ],
      "source": [
        "# TODO\n",
        "# yhat = ... (vector containing predicted 0,1 labels)\n",
        "# acc = np.mean(yhat == yts8)\n",
        "# print(\"Test accuracy = %f\" % acc)\n",
        "\n",
        "yhat = Ktest@alpha\n",
        "print(yhat)\n",
        "print(yts8)\n",
        "yhat = np.where(yhat <= 0,0,1)\n",
        "# np.where(x < y, x, 10 + y)\n",
        "acc = np.mean(yhat == yts8)\n",
        "print(\"Test accuracy = %f\" % acc)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2SO0yhM_pPv"
      },
      "source": [
        "## Kernel Support Vector Machine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9x1jZnh_pPv"
      },
      "source": [
        "The goal of this lab is to compare Kernel Logistic Regression to Kernel Support Vector machines. Following `demo_mnist_svm.ipynb` create and train an SVM classifier on `Xtr1` and `ytr8` using an RBF kernel with `gamma = .05` (the same value we used for logistic regression aboe). Use margin parameter `C = 10`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "7r-c_g2z_pPv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08fa34df-e928-492f-fc05-b5cd59aa5109"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LibSVM]"
          ]
        }
      ],
      "source": [
        "# TODO\n",
        "\n",
        "# gamma = [0.005]\n",
        "t = time.time()\n",
        "svcrbf = svm.SVC(probability= False, kernel = \"rbf\", C = 10, gamma = 0.05, verbose = 10)\n",
        "svcrbf.fit(Xtr1, ytr8)\n",
        "yhat_ts = svcrbf.predict(Xts1)\n",
        "ysvm = yhat_ts\n",
        "acc = np.mean(ysvm == yts8)\n",
        "elapsed = time.time() - 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hcOkVNj_pPw"
      },
      "source": [
        "Calculate and print the accuracy of the SVM classifier. You should obtain a similar result as for logistic regression: something close to $97\\%$ accuracy. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "o5nSyO3J_pPw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "941fbf61-ad23-4853-fffa-96cdf6aed8f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy = 0.977600\n"
          ]
        }
      ],
      "source": [
        "# TODO\n",
        "# ysvm = ... (vector containing predicted 0,1 labels)\n",
        "yhat_ts = svcrbf.predict(Xts1)\n",
        "ysvm = yhat_ts\n",
        "acc = np.mean(yhat == yts8)\n",
        "print(\"Test accuracy = %f\" % acc)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWp9NlMa_pPw"
      },
      "source": [
        "## The Showdown \n",
        "\n",
        "Both SVM classifiers and kernel logisitic regression require tuning parameters to obtain the best possible result. In our setting we will stick with an RBF kernel (although this could be tuned). So we only consider tuning the kernel width parameter `gamma`, as well as the regularization parameter `lamb` for logistic regression, and the margin parameter `C` for SVM. We will choose parameters using for-loops and train-test cross validation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ia44LYbc_pPw"
      },
      "source": [
        "Train a logistic regression classifier with **all combinations** of the parameters included below in vectors `gamma` and `lamb`. For each setting of parameters, compute and print:\n",
        "* the test error obtained\n",
        "* the total runtime of classification in seconds (including training time and prediction time)\n",
        "\n",
        "For computing runtime you might want to use the `time()` function from the `time` library, which we already imported ealier. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cHNr0Kw_pPw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c897641-8d56-4cfb-e323-fcd869604c41"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: OptimizeWarning: Unknown solver options: maxIter\n",
            "  \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gamma is 0.1\n",
            "Lamb is 0\n",
            "The acc is 0.9634\n",
            "The runtime is 50.194396018981934\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: OptimizeWarning: Unknown solver options: maxIter\n",
            "  \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gamma is 0.1\n",
            "Lamb is 1e-06\n",
            "The acc is 0.9658\n",
            "The runtime is 154.05476665496826\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: OptimizeWarning: Unknown solver options: maxIter\n",
            "  \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gamma is 0.1\n",
            "Lamb is 0.0001\n",
            "The acc is 0.9652\n",
            "The runtime is 254.28274488449097\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: OptimizeWarning: Unknown solver options: maxIter\n",
            "  \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gamma is 0.1\n",
            "Lamb is 0.01\n",
            "The acc is 0.9628\n",
            "The runtime is 362.0733976364136\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: OptimizeWarning: Unknown solver options: maxIter\n",
            "  \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gamma is 0.05\n",
            "Lamb is 0\n",
            "The acc is 0.981\n",
            "The runtime is 82.08323788642883\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: OptimizeWarning: Unknown solver options: maxIter\n",
            "  \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gamma is 0.05\n",
            "Lamb is 1e-06\n",
            "The acc is 0.98\n",
            "The runtime is 319.41613960266113\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: OptimizeWarning: Unknown solver options: maxIter\n",
            "  \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gamma is 0.05\n",
            "Lamb is 0.0001\n",
            "The acc is 0.978\n",
            "The runtime is 578.5552973747253\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: OptimizeWarning: Unknown solver options: maxIter\n",
            "  \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gamma is 0.05\n",
            "Lamb is 0.01\n",
            "The acc is 0.9742\n",
            "The runtime is 963.3965771198273\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: OptimizeWarning: Unknown solver options: maxIter\n",
            "  \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gamma is 0.02\n",
            "Lamb is 0\n",
            "The acc is 0.9838\n",
            "The runtime is 211.5088665485382\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: OptimizeWarning: Unknown solver options: maxIter\n",
            "  \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gamma is 0.02\n",
            "Lamb is 1e-06\n",
            "The acc is 0.985\n",
            "The runtime is 1879.7416179180145\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: OptimizeWarning: Unknown solver options: maxIter\n",
            "  \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gamma is 0.02\n",
            "Lamb is 0.0001\n",
            "The acc is 0.9844\n",
            "The runtime is 3808.1090004444122\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: OptimizeWarning: Unknown solver options: maxIter\n",
            "  \n"
          ]
        }
      ],
      "source": [
        "gamma = [.1, .05,.02,.01,.005]\n",
        "lamb = [0,1e-6,1e-4,1e-2]\n",
        "# TODO\n",
        "# ...\n",
        "for i in range(len(gamma)): \n",
        "  t = time.time()\n",
        "  K = pairwise_kernels(Xtr1, Xtr1, metric = \"rbf\", gamma = gamma[i])\n",
        "  for j in range(len(lamb)):\n",
        "    alpha = log_fit(K, ytr8, lamb[j], nit = 1000)\n",
        "    Ktest = pairwise_kernels(Xts1, Xtr1, metric = \"rbf\", gamma = gamma[i])\n",
        "    # yStorage = np.zeros(yts8.shape[0])\n",
        "    yhat = np.zeros(yts8.shape[0])\n",
        "    for k in range(yts8.shape[0]):\n",
        "      curr_yhat = alpha@Ktest[k, :]\n",
        "      if curr_yhat >0: \n",
        "        yhat[k] = 1 \n",
        "    acc = np.mean(yhat == yts8)\n",
        "    elapsed = time.time() - t \n",
        "    print('Gamma is ' + str(gamma[i]))\n",
        "    print('Lamb is ' + str(lamb[j]))\n",
        "    print('The acc is ' + str(acc))\n",
        "    print('The runtime is ' + str(elapsed))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkcP20AM_pPw"
      },
      "source": [
        "TODO: What was the best test error achieved, and what setting of parameters achieved this error? Was the kernel logistic regression classifier more sensitive to changes in `gamma` or `lamb`? Discuss in 1-3 short sentences below.\n",
        "\n",
        "Best performance error achieved: 0.98. \n",
        "\n",
        "It is more sensitive to gamma, because the acc rises as gamma becomes smaller. Whereas for changes in lamb, it is usually changes too, but not as significantly as what gamma does to acc. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHxF0MXm_pPw"
      },
      "source": [
        "Now let's do the same thing for the kernel Support Vector Classifier. Train an SVM classifier with **all combinations** of the parameters included below in vectors `gamma` and `C`. For each setting of parameters, compute:\n",
        "* the test error obtained\n",
        "* the total runtime of classification in seconds (including training time and prediction time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "FkPkZGIY_pPx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c960424-d266-4fd8-c0c6-abbc8c807acf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LibSVM]Gamma is 0.1\n",
            "Lamb is 0\n",
            "The acc is 0.9022\n",
            "The runtime is 74.9006564617157\n",
            "[LibSVM]Gamma is 0.1\n",
            "Lamb is 1e-06\n",
            "The acc is 0.9022\n",
            "The runtime is 184.75584721565247\n",
            "[LibSVM]Gamma is 0.1\n",
            "Lamb is 0.0001\n",
            "The acc is 0.9282\n",
            "The runtime is 321.3443694114685\n",
            "[LibSVM]Gamma is 0.1\n",
            "Lamb is 0.01\n",
            "The acc is 0.9342\n",
            "The runtime is 476.0332658290863\n",
            "[LibSVM]Gamma is 0.05\n",
            "Lamb is 0\n",
            "The acc is 0.9022\n",
            "The runtime is 26.92007875442505\n",
            "[LibSVM]Gamma is 0.05\n",
            "Lamb is 1e-06\n",
            "The acc is 0.9228\n",
            "The runtime is 53.38871169090271\n",
            "[LibSVM]Gamma is 0.05\n",
            "Lamb is 0.0001\n",
            "The acc is 0.9774\n",
            "The runtime is 84.71781659126282\n",
            "[LibSVM]Gamma is 0.05\n",
            "Lamb is 0.01\n",
            "The acc is 0.9802\n",
            "The runtime is 118.57894682884216\n",
            "[LibSVM]Gamma is 0.02\n",
            "Lamb is 0\n",
            "The acc is 0.9022\n",
            "The runtime is 16.961200952529907\n",
            "[LibSVM]Gamma is 0.02\n",
            "Lamb is 1e-06\n",
            "The acc is 0.9512\n",
            "The runtime is 31.888372659683228\n",
            "[LibSVM]Gamma is 0.02\n",
            "Lamb is 0.0001\n",
            "The acc is 0.982\n",
            "The runtime is 43.28492593765259\n",
            "[LibSVM]Gamma is 0.02\n",
            "Lamb is 0.01\n",
            "The acc is 0.9858\n",
            "The runtime is 55.36175560951233\n",
            "[LibSVM]Gamma is 0.01\n",
            "Lamb is 0\n",
            "The acc is 0.9022\n",
            "The runtime is 16.699944734573364\n",
            "[LibSVM]Gamma is 0.01\n",
            "Lamb is 1e-06\n",
            "The acc is 0.945\n",
            "The runtime is 31.225494623184204\n",
            "[LibSVM]Gamma is 0.01\n",
            "Lamb is 0.0001\n",
            "The acc is 0.976\n",
            "The runtime is 41.279560565948486\n",
            "[LibSVM]Gamma is 0.01\n",
            "Lamb is 0.01\n",
            "The acc is 0.987\n",
            "The runtime is 50.9332857131958\n",
            "[LibSVM]Gamma is 0.005\n",
            "Lamb is 0\n",
            "The acc is 0.9022\n",
            "The runtime is 16.181135892868042\n",
            "[LibSVM]Gamma is 0.005\n",
            "Lamb is 1e-06\n",
            "The acc is 0.932\n",
            "The runtime is 31.721287965774536\n",
            "[LibSVM]Gamma is 0.005\n",
            "Lamb is 0.0001\n",
            "The acc is 0.9672\n",
            "The runtime is 42.53058862686157\n",
            "[LibSVM]Gamma is 0.005\n",
            "Lamb is 0.01\n",
            "The acc is 0.9838\n",
            "The runtime is 51.03973126411438\n"
          ]
        }
      ],
      "source": [
        "gamma = [.1, .05,.02,.01,.005]\n",
        "C = [.01,.1,1,10]\n",
        "\n",
        "for i in range(len(gamma)):\n",
        "  t = time.time()\n",
        "  for j in range(len(C)):\n",
        "    svcrbf = svm.SVC(probability=False, kernel = \"rbf\", C = C[j], gamma = gamma[i], verbose = 10)\n",
        "    svcrbf.fit(Xtr1, ytr8)\n",
        "    yhat_ts = svcrbf.predict(Xts1)\n",
        "    ysvm = yhat_ts\n",
        "    acc = np.mean(ysvm == yts8)\n",
        "    elapsed = time.time() - t\n",
        "    print('Gamma is ' + str(gamma[i]))\n",
        "    print('Lamb is ' + str(lamb[j]))\n",
        "    print('The acc is ' + str(acc))\n",
        "    print('The runtime is ' + str(elapsed))\n",
        "# TODO\n",
        "# ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPI-k50w_pPx"
      },
      "source": [
        "TODO: What was the best test error achieved, and what setting of parameters achieved this error? Which performed better in terms of accuracy, the SVM or logisitic regression classifier? How about in terms of runtime?\n",
        "\n",
        "The best test error achieved: 0.987\n",
        "\n",
        "The setting of parameters: \n",
        "[LibSVM]Gamma is 0.01\n",
        "Lamb is 0.01\n",
        "The acc is 0.987\n",
        "The runtime is 50.9332857131958\n",
        "\n",
        "The logistic regression performed better, but in terms of runtime, the SVM performed better. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wec-DXhg_pPx"
      },
      "source": [
        "**NOTE:** For `sklearns`'s built in classifiers, including svm.SVC, there is a function called `GridSearchCV` which can automatically perform hyperparamater tuning for you. The main advantage of the method (as opposed to writing for-loops) is that it supports parallelization, so it can fit with different parameters at the same time. The function also supports automatic $k$-fold cross-validation (instead of simple train/test split). \n",
        "\n",
        "You might be interested in using this function in the future. If so, please check out the tutorial in the following lab from previous year: https://github.com/sdrangan/introml/blob/master/unit08_svm/lab_emnist_partial.ipynb."
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "collapsed_sections": [],
      "name": "lab4.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}